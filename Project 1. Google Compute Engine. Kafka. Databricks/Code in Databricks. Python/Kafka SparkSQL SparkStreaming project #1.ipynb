{"cells":[{"cell_type":"markdown","source":["# Streaming data from Kafka and analyzing realtime / Language:python"],"metadata":{}},{"cell_type":"markdown","source":["## Creating connection with Kafka server on Google Compute Engine"],"metadata":{}},{"cell_type":"code","source":["import string\nfrom pyspark.sql.functions import *\n\ndf = spark \\\n  .readStream \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"35.193.247.113:9092\") \\\n  .option(\"subscribe\", \"ripple-api-v4\") \\\n  .option(\"startingOffsets\", \"latest\") \\\n  .load() \\\n  .selectExpr(\"CAST(value AS STRING)\")\n  \ninterm = df.select(get_json_object(df[\"value\"],\"$.payload\").alias('value'))\nres = interm.select(get_json_object(interm[\"value\"],\"$.time\").cast(\"float\").cast(\"timestamp\").alias(\"time\"),\n\t\t   get_json_object(interm[\"value\"],\"$.amount\").cast(\"float\").alias(\"amount\"),\n\t\t   get_json_object(interm[\"value\"],\"$.currency\").alias(\"currency\"),\n\t\t   get_json_object(interm[\"value\"],\"$.t_hash\").alias(\"t_hash\"))\n\ndisplay(res)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###Let's Add Static Dataframe with Countries and currency"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\narr_cur_countries = [('CNY','CHN'),('USD','USA'),('JPY','JPN'),('INR','IND')]\nrdd = sc.parallelize(arr_cur_countries)\ncur_countries = rdd.map(lambda x: Row(currency=x[0], country=x[1]))\nschemaCurCountries = sqlContext.createDataFrame(cur_countries)\n\ndisplay(schemaCurCountries)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Stream Processing, adding 1 second \"Window\" to our Data\n## and join Streaming DataFrame with Static DataFrame"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Same query as staticInputDF\nstreamingCountsDF = (\n  res\n    .groupBy(\n      res.t_hash, \n      res.currency, \n      res.amount, \n     window(res.time, \"1 second\"))\n   .count()\n)\n\n#JOINING STREAMINGDATAFRAME WITH STATIC DATAFRAME\nstreamingCountsDF = streamingCountsDF.join(schemaCurCountries, \"currency\", \"left\")\ndisplay(streamingCountsDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#Lets store our dataframe in memory and do some Magic!"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["###Show me the Relative amount each second!"],"metadata":{}},{"cell_type":"code","source":["%sql select date_format(window.end, \"hh:mm:ss\") as time, count, amount, currency from counts order by time"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["As we can see money is flowing over a period, streaming..."],"metadata":{}},{"cell_type":"code","source":["%sql select amount, currency, date_format(window.end, \"hh:mm:ss\") as time, count from counts order by time"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["That chart shows us that INDIA sends a lot of money in ONE transaction, we are using country variable from static dataframe"],"metadata":{}},{"cell_type":"code","source":["%sql select amount, country, date_format(window.end, \"MMM-dd HH:mm:ss\") as time from counts where country is not null order by time"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["###Amount on packets coming each second..."],"metadata":{}},{"cell_type":"code","source":["%sql select currency, date_format(window.end, \"MMM-dd HH:mm:ss\") as time, count from counts order by time, currency"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#Thank you!"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"Kafka SparkSQL SparkStreaming project #1","notebookId":725022155228800},"nbformat":4,"nbformat_minor":0}
